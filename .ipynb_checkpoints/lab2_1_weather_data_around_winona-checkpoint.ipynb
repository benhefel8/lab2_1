{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxJucrKj7mHJ"
   },
   "source": [
    "# Lab 2.1 - Weather Data Around Winona\n",
    "\n",
    "In this lab, we will download and combine a decades worth of weather data from the NOAA, focusing on weather stations within 500 miles of Winona.\n",
    "\n",
    "Here is the outline of the basic process.\n",
    "\n",
    "1. Install and investigate useful packages.\n",
    "2. Find all weather stations in proximity to Winona.\n",
    "3. Use a single station to prototype our tools.\n",
    "4. Automate the process of downloading and uncompressing data from all stations of interest.\n",
    "5. Output the results to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeicFuT-8Vnx"
   },
   "source": [
    "## Problem 1 - Install and investigate useful tools.\n",
    "\n",
    "First, you should install and investigate the following tools.\n",
    "\n",
    "1. **`wget`** is a tool for programmically downloading data files from the web on the command line.  There is a Python wrapper to this tool that you can install with `pip` as shown below.\n",
    "2. **`geopy`** is a package that, among other things, implements a function for computing distances between two lat-long pairs. Again, install this package with `pip` as shown below.\n",
    "3. **`gzip`** is part of the standard Python library and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c8pRv9PyCvPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uPWEksjS9REC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQXgqoPb9e53"
   },
   "source": [
    "#### Task 1.1 - Investigate using `wget` to download a file.\n",
    "\n",
    "Read the help/documentation on `wget` to figure out how to download the following data file [Some random data file from STAT 210] into the `./data` sub-folder.\n",
    "\n",
    "[https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv](https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D92NkCq_995Z"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'wget' from 'C:\\\\Users\\\\hp6265bz\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\polars\\\\Lib\\\\site-packages\\\\wget.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages\\wget.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Download utility as an easy way to get file from the net\n",
       " \n",
       "  python -m wget <URL>\n",
       "  python wget.py <URL>\n",
       "\n",
       "Downloads: http://pypi.python.org/pypi/wget/\n",
       "Development: http://bitbucket.org/techtonik/python-wget/\n",
       "\n",
       "wget.py is not option compatible with Unix wget utility,\n",
       "to make command line interface intuitive for new people.\n",
       "\n",
       "Public domain by anatoly techtonik <techtonik@gmail.com>\n",
       "Also available under the terms of MIT license\n",
       "Copyright (c) 2010-2015 anatoly techtonik"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/yardsale8/STAT_210/raw/refs/heads/main/data/sars1.csv\"\n",
    "output = \"./data/sars1.csv\"\n",
    "file_name = wget.download(url, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTW9hZXX-Ceq"
   },
   "source": [
    "#### Task 1.2 - Investigate using `geopy.distance.distance` to compute a distance in miles.\n",
    "\n",
    "1. Import the `distance` function from the `geopy.distance` submodule.\n",
    "2. Use Wikipedia to find the lat-long coordinates of Winona and Rochester MN.\n",
    "3. Use `distance` to compute the distance between Winona and Rochester.\n",
    "4. Use some other source (e.g., Google Maps) to check the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m-XZZQ2v-i4t"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Calculate the geodesic distance between points.\n",
       "\n",
       "Set which ellipsoidal model of the earth to use by specifying an\n",
       "``ellipsoid`` keyword argument. The default is 'WGS-84', which is the\n",
       "most globally accurate model.  If ``ellipsoid`` is a string, it is\n",
       "looked up in the `ELLIPSOIDS` dictionary to obtain the major and minor\n",
       "semiaxes and the flattening. Otherwise, it should be a tuple with those\n",
       "values.  See the comments above the `ELLIPSOIDS` dictionary for\n",
       "more information.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> from geopy.distance import geodesic\n",
       "    >>> newport_ri = (41.49008, -71.312796)\n",
       "    >>> cleveland_oh = (41.499498, -81.695391)\n",
       "    >>> print(geodesic(newport_ri, cleveland_oh).miles)\n",
       "    538.390445368\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "There are 3 ways to create a distance:\n",
       "\n",
       "- From kilometers::\n",
       "\n",
       "    >>> from geopy.distance import Distance\n",
       "    >>> Distance(1.42)\n",
       "    Distance(1.42)\n",
       "\n",
       "- From units::\n",
       "\n",
       "    >>> from geopy.distance import Distance\n",
       "    >>> Distance(kilometers=1.42)\n",
       "    Distance(1.42)\n",
       "    >>> Distance(miles=1)\n",
       "    Distance(1.609344)\n",
       "\n",
       "- From points (for non-abstract distances only),\n",
       "  calculated as a sum of distances between all points::\n",
       "\n",
       "    >>> from geopy.distance import geodesic\n",
       "    >>> geodesic((40, 160), (40.1, 160.1))\n",
       "    Distance(14.003702498106215)\n",
       "    >>> geodesic((40, 160), (40.1, 160.1), (40.2, 160.2))\n",
       "    Distance(27.999954644813478)\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\site-packages\\geopy\\distance.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.54418575388878"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winona = (44.050556, -91.668333)\n",
    "rochester = (44.023333, -92.461389)\t\n",
    "distance(winona, rochester).miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHdoE7ZFBXlU"
   },
   "source": [
    "#### Task 1.3 - Investigate `gzip`\n",
    "\n",
    "The yearly NOAA data is compressed as `.gz` files, which need to be uncompressed using `gzip`.  Explore the `gzip` module by\n",
    "\n",
    "1. Exploring the documentation/help for the `gzip` module,\n",
    "2. Using `wget` to download the following link into the `./data` folder, and\n",
    "3. Using `gzip` to uncompress this file.\n",
    "4. Inspect the data in your list, which should be of type `byte`.  Use a comprehension with the expression `l.decode('utf-8')` to convert this to a list of strings.\n",
    "5. Write the uncompressed lines to an output file using `with open(path, 'w') as out` and the `writelines` method of `out`.  \n",
    "\n",
    "**Link.** [https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz](https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "88NVuihyCBBO"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        module\n",
       "\u001b[1;31mString form:\u001b[0m <module 'gzip' from 'C:\\\\Users\\\\hp6265bz\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\polars\\\\Lib\\\\gzip.py'>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\hp6265bz\\appdata\\local\\anaconda3\\envs\\polars\\lib\\gzip.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Functions that read and write gzipped files.\n",
       "\n",
       "The user of the file doesn't have to worry about the compression,\n",
       "but random access is not allowed."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz\"\n",
    "output = \"./data/1750.csv.gz\"\n",
    "file_name = wget.download(url, \"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " b'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " b'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " b'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " b'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " b'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open(\"./data/1750.csv.gz\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASN00002061,17500201,PRCP,56,,,a,\\n',\n",
       " 'ASN00003014,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003059,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00003088,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009015,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009193,17500201,TMIN,187,,,a,\\n',\n",
       " 'ASN00009193,17500201,PRCP,0,,,a,\\n',\n",
       " 'ASN00009500,17500201,DATX,2,,,a,\\n',\n",
       " 'ASN00009500,17500201,MDTX,210,,,a,\\n',\n",
       " 'ASN00009592,17500201,DATX,4,,,a,\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = [l.decode(\"utf-8\") for l in lines]\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"./data/1750.csv\", \"w\") as f:\n",
    "    out = f.writelines(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt1MlIz9DWpm"
   },
   "source": [
    "## Problem 2 - Find all stations within 500 miles of Winona, MN.\n",
    "\n",
    "The file linked below contains information about all stations tracked by NOAA.  \n",
    "\n",
    "*Main folder:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/\n",
    "\n",
    "*Station txt file:* https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
    "\n",
    "*Note.* While it would be easier to use the CSV version of the station file, you should use the TXT version here (for practice).\n",
    "\n",
    "**Your tasks** Our goal is to get a list of stations that are within 500 miles of Winona.  Do this by\n",
    "\n",
    "1. Using `wget` to download the stations information into the `./data` folder.\n",
    "2. Use `with` to read the lines of this file.\n",
    "3. At this point, the lines are strings in a fixed-width format separated by whitespace.  Use a list comprehension with the string split method to split the raw lines (strings) into a list of entries.\n",
    "4. There are three entries of interest, the station ID and the lat-long coordinates of the station.  Inspect the file to determine the index for these three entries.\n",
    "5. We want to transform the lines (currently a list of strings) into a record, which is a `dict` with good names for the entries as keys and the values representing the data in an appropriate type (string for station ID, `float` for the lat-long).  Use a comprehension to create a list of records as described.\n",
    "6. Use another comprehension to apply a filter to the stations, keeping only those within 500 miles of Winona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F0J443KoFTrs"
   },
   "outputs": [],
   "source": [
    "# Your code here (add cells as needed)\n",
    "url =  \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "output = \"./data/stations.txt\"\n",
    "file_name = wget.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACW00011604  17.1167  -61.7833   10.1    ST JOHNS COOLIDGE FLD                       \\n',\n",
       " 'ACW00011647  17.1333  -61.7833   19.2    ST JOHNS                                    \\n',\n",
       " 'AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\\n',\n",
       " 'AEM00041194  25.2550   55.3640   10.4    DUBAI INTL                             41194\\n',\n",
       " 'AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                         41217\\n',\n",
       " 'AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                            41218\\n',\n",
       " 'AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG                   GSN     40930\\n',\n",
       " 'AFM00040938  34.2100   62.2280  977.2    HERAT                                  40938\\n',\n",
       " 'AFM00040948  34.5660   69.2120 1791.3    KABUL INTL                             40948\\n',\n",
       " 'AFM00040990  31.5000   65.8500 1010.0    KANDAHAR AIRPORT                       40990\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"./data/stations.txt\") as f:\n",
    "    raw_lines = f.readlines()\n",
    "raw_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ACW00011604',\n",
       "  '17.1167',\n",
       "  '-61.7833',\n",
       "  '10.1',\n",
       "  'ST',\n",
       "  'JOHNS',\n",
       "  'COOLIDGE',\n",
       "  'FLD'],\n",
       " ['ACW00011647', '17.1333', '-61.7833', '19.2', 'ST', 'JOHNS'],\n",
       " ['AE000041196',\n",
       "  '25.3330',\n",
       "  '55.5170',\n",
       "  '34.0',\n",
       "  'SHARJAH',\n",
       "  'INTER.',\n",
       "  'AIRP',\n",
       "  'GSN',\n",
       "  '41196'],\n",
       " ['AEM00041194', '25.2550', '55.3640', '10.4', 'DUBAI', 'INTL', '41194'],\n",
       " ['AEM00041217',\n",
       "  '24.4330',\n",
       "  '54.6510',\n",
       "  '26.8',\n",
       "  'ABU',\n",
       "  'DHABI',\n",
       "  'INTL',\n",
       "  '41217']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines = [line.split() for line in raw_lines]\n",
    "split_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The indexes for station ID and lat-long, are 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Station ID': 'ACW00011604', 'Latitude': 17.1167, 'Longitude': -61.7833},\n",
       " {'Station ID': 'ACW00011647', 'Latitude': 17.1333, 'Longitude': -61.7833},\n",
       " {'Station ID': 'AE000041196', 'Latitude': 25.333, 'Longitude': 55.517},\n",
       " {'Station ID': 'AEM00041194', 'Latitude': 25.255, 'Longitude': 55.364},\n",
       " {'Station ID': 'AEM00041217', 'Latitude': 24.433, 'Longitude': 54.651},\n",
       " {'Station ID': 'AEM00041218', 'Latitude': 24.262, 'Longitude': 55.609},\n",
       " {'Station ID': 'AF000040930', 'Latitude': 35.317, 'Longitude': 69.017},\n",
       " {'Station ID': 'AFM00040938', 'Latitude': 34.21, 'Longitude': 62.228},\n",
       " {'Station ID': 'AFM00040948', 'Latitude': 34.566, 'Longitude': 69.212},\n",
       " {'Station ID': 'AFM00040990', 'Latitude': 31.5, 'Longitude': 65.85}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rows = [{\n",
    "        \"Station ID\": parts[0],\n",
    "        \"Latitude\": float(parts[1]),\n",
    "        \"Longitude\": float(parts[2])\n",
    "    }\n",
    "    for parts in split_lines\n",
    "]\n",
    "dict_rows[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import distance\n",
    "\n",
    "winona = (44.050556, -91.668333)\n",
    "\n",
    "filtered_stations = [row for row in dict_rows\n",
    "                        if distance(winona, (row[\"Latitude\"], row[\"Longitude\"])).miles <= 25.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7_9ve26IduT"
   },
   "source": [
    "#### Problem 3 - Prototype downloading and uncompressing a station file.\n",
    "\n",
    "Before we download and uncompress all the stations of interest, let's practice on one station file.\n",
    "\n",
    "\n",
    "1. Copy the url for some station and store is as a variable named `url`.\n",
    "2. Write `lambda` functions that extract each of the following from the station `url`: compressed file name, compressed file path (e.g., `./data/...`), and uncompressed file path (e.g., `./data/...`).\n",
    "3. Write a `lambda` function that extracts\n",
    "4. Use `wget` to download this stations data.\n",
    "5. Use `gzip` to uncompress the data.\n",
    "6. Write the data to out output file.\n",
    "\n",
    "Your code should have the following shape:\n",
    "\n",
    "```{Python}\n",
    "wget.download(...)\n",
    "with gzip.open(...) as f:\n",
    "    with open(..., 'w') as out:\n",
    "        f.readlines()\n",
    "        out.writelines(f)\n",
    "```\n",
    "\n",
    "You should be using your helper functions to, in part, fill in the `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "04YS60A1JciS"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/1750.csv.gz\"\n",
    "\n",
    "get_compressed_filename = lambda u: u.split(\"/\")[-1]\n",
    "get_compressed_path = lambda fname: f\"./data/{fname}\"\n",
    "get_uncompressed_path = lambda fname: f\"./data/{fname.replace('.gz','')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1750.csv.gz'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_filename = get_compressed_filename(url)\n",
    "compressed_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/1750.csv.gz'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_filepath = get_compressed_path(compressed_filename)\n",
    "compressed_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/1750.csv'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncompressed_filepath = get_uncompressed_path(compressed_filename)\n",
    "uncompressed_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/1750.csv (1).gz'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = wget.download(url, out=compressed_filepath)\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download(url, out = compressed_filepath)\n",
    "with gzip.open(compressed_filepath, \"rt\") as f:\n",
    "    with open(uncompressed_filepath, \"w\") as out:\n",
    "        f.readline()\n",
    "        out.writelines(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzgFqv5VF38i"
   },
   "source": [
    "## Problem 4 - Build the station URLs and download the files.\n",
    "\n",
    "**Tasks.** Now you need to build urls for all stations of interest by\n",
    "\n",
    "1. Use a comprehension to extract the stations of interest into a list.\n",
    "2. Investigating the structure of the files stored in the `by_station` folder (see main folder link above).\n",
    "3. Use a comprehension and an `f` string to build a list of URLS for all stations of interest.\n",
    "4. Use `wget` to download the data for the stations of interest into the data folder.\n",
    "5. Use `gzip` to uncompress the files.\n",
    "6. Convert the `bytes` to `str` of format `utf-8`.\n",
    "7. Use the append mode `\"a\"` of `open` with `writelines` to append the data in each file to your output file.\n",
    "\n",
    "While we usually avoid using a `for` loop, we make an exception for code for lengthy IO.  To accomplish steps 4 & 5, use a `for` loop with the following shape.\n",
    "\n",
    "```{Python}\n",
    "for url in station_urls:\n",
    "    wget.download(...)\n",
    "    with gzip.open(...) as f:\n",
    "        with open(..., 'a') as out:\n",
    "            f.readlines()\n",
    "            ... # Convert lines to strings here\n",
    "            out.writelines(f)\n",
    "    print(f\"Downloaded and extracted the data for {url}\")\n",
    "```\n",
    "\n",
    "Note that the code inside the loop should resemble the code from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SOUt7rCBIZ6f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US1MNHS0001', 'US1MNHS0006', 'US1MNHS0007', 'US1MNHS0008', 'US1MNHS0009']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here.\n",
    "station_ids = [row[\"Station ID\"] for row in filtered_stations]\n",
    "station_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0006.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0007.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0008.csv.gz',\n",
       " 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0009.csv.gz']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station\"\n",
    "station_urls = [f\"{base_url}/{sid}.csv.gz\" for sid in station_ids]\n",
    "station_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./data/all_stations_data.csv\" \n",
    "# adding a header\n",
    "header = \"ID,DATE,ELEMENT,DATA_VALUE,M_FLAG,Q_FLAG,S_FLAG,OBS_TIME\\n\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0001.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0012.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0022.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNHS0028.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0038.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNOL0991.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWB0015.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0006.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0007.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0009.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0013.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0019.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0021.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0023.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0024.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0026.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0027.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0029.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0031.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1MNWN0033.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WIBF0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WIBF0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0011.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0018.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0022.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WILC0038.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0002.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0003.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0004.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0005.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0008.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/US1WITR0010.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210146.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00210559.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00213812.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00214418.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00215488.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217184.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00217277.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00218951.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219067.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219072.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00219077.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00470124.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472165.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472992.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00472996.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00474366.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USC00478589.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00004956.csv.gz\n",
      "Downloaded and extracted the data for https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_station/USW00014920.csv.gz\n"
     ]
    }
   ],
   "source": [
    "compressed_filename = url.split(\"/\")[-1]             \n",
    "compressed_filepath = f\"./data/{compressed_filename}\" \n",
    "\n",
    "for url in station_urls:\n",
    "    wget.download(url, out = compressed_filepath)\n",
    "    with gzip.open(compressed_filepath, \"rt\", encoding = \"utf-8\") as f:\n",
    "        with open(output_file, \"a\", encoding = \"utf-8\") as out:\n",
    "            lines = f.readlines()\n",
    "            out.writelines(lines)\n",
    "    print(f\"Downloaded and extracted the data for {url}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
